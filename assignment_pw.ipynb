{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " 1.What is Boosting in Machine Learning? Explain how it improves weak\n",
        " learners\n",
        "\n",
        "\n",
        "->Boosting in Machine Learning is an ensemble learning technique that combines many weak learners to create a strong learner.\n",
        "\n",
        "What is a Weak Learner?\n",
        "\n",
        "A weak learner is a model that performs only slightly better than random guessing\n",
        "(e.g., a simple decision tree with one split).\n",
        "\n",
        "What is Boosting?\n",
        "\n",
        "Boosting trains weak learners one after another (sequentially).\n",
        "Each new learner focuses more on the mistakes made by the previous learners.\n",
        "\n",
        "How Boosting Improves Weak Learners\n",
        "\n",
        "Sequential Learning\n",
        "\n",
        "Models are trained one by one, not independently.\n",
        "\n",
        "Focus on Errors\n",
        "\n",
        "After each model, more importance (weight) is given to misclassified data points.\n",
        "\n",
        "The next model tries harder to correctly predict these difficult cases.\n",
        "\n",
        "Weighted Combination\n",
        "\n",
        "All weak learners are combined using weights.\n",
        "\n",
        "Better-performing learners get higher weight in the final prediction.\n",
        "\n",
        "Reduced Bias and Error\n",
        "\n",
        "By correcting mistakes repeatedly, boosting reduces bias and improves accuracy.\n",
        "\n",
        "Simple Example\n",
        "\n",
        "Imagine classifying emails as spam or not spam:\n",
        "\n",
        "First model makes mistakes.\n",
        "\n",
        "Second model focuses more on the wrongly classified emails.\n",
        "\n",
        "Third model improves further.\n",
        "\n",
        "Final decision is made by combining all models.\n",
        "\n",
        "Result: Higher accuracy than any single weak model.\n",
        "\n",
        "Popular Boosting Algorithms\n",
        "\n",
        "AdaBoost (Adaptive Boosting)\n",
        "\n",
        "Gradient Boosting\n",
        "\n",
        "XGBoost\n",
        "\n",
        "LightGBM\n",
        "\n",
        "CatBoost"
      ],
      "metadata": {
        "id": "9JjAFS9tzkuc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.What is the difference between AdaBoost and Gradient Boosting in terms\n",
        "of how models are trained?\n",
        "\n",
        "\n",
        "->Difference between AdaBoost and Gradient Boosting\n",
        "\n",
        "(Based on how models are trained ‚Äì short & easy)\n",
        "\n",
        "Aspect\tAdaBoost\tGradient Boosting\n",
        "Training approach\tTrains models sequentially by changing weights of data points\tTrains models sequentially by fitting on errors (residuals)\n",
        "Focus during training\tFocuses more on misclassified samples by increasing their weights\tFocuses on reducing overall loss using gradient descent\n",
        "Error handling\tUses sample weights to correct mistakes\tUses residual errors from previous model\n",
        "Loss function\tUses a fixed exponential loss function\tCan use any differentiable loss function\n",
        "Model contribution\tEach model is given a weight based on its accuracy\tEach model contributes by adding predictions to reduce error\n",
        "Flexibility\tLess flexible\tMore flexible and powerful"
      ],
      "metadata": {
        "id": "O4T7r7wgzxqV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.How does regularization help in XGBoost?\n",
        "\n",
        "->How Regularization Helps in XGBoost\n",
        "\n",
        "(Short & easy explanation)\n",
        "\n",
        "Regularization in XGBoost helps to prevent overfitting and makes the model generalize better on new (unseen) data.\n",
        "\n",
        "Why Regularization is Needed\n",
        "\n",
        "XGBoost builds many trees.\n",
        "If trees become too complex, the model may:\n",
        "\n",
        "Fit noise in training data\n",
        "\n",
        "Perform poorly on test data\n",
        "\n",
        "Regularization controls this complexity.\n",
        "\n",
        "Types of Regularization in XGBoost\n",
        "1. Tree Complexity Regularization\n",
        "\n",
        "Penalizes large trees\n",
        "\n",
        "Controls:\n",
        "\n",
        "Number of leaves\n",
        "\n",
        "Depth of trees\n",
        "\n",
        "Parameters:\n",
        "\n",
        "Œ≥ (gamma) ‚Üí minimum loss reduction needed to split a node\n",
        "\n",
        "max_depth ‚Üí limits tree depth\n",
        "\n",
        "üëâ Result: Simpler trees\n",
        "\n",
        "2. L1 and L2 Regularization on Leaf Weights\n",
        "\n",
        "L1 (alpha) ‚Üí encourages sparsity (some leaf weights become zero)\n",
        "\n",
        "L2 (lambda) ‚Üí keeps leaf weights small and smooth\n",
        "\n",
        "üëâ Result: Prevents extreme predictions\n",
        "\n",
        "How It Improves Model Performance\n",
        "\n",
        "Reduces overfitting\n",
        "\n",
        "Improves prediction stability\n",
        "\n",
        "Makes the model more robust to noise\n",
        "\n",
        "Enhances generalization ability"
      ],
      "metadata": {
        "id": "dpeRHxbt0OdT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.Why is CatBoost considered efficient for handling categorical data?\n",
        "\n",
        "->CatBoost is considered efficient for handling categorical data because it is designed to work with categorical features directly and safely, without heavy preprocessing.\n",
        "\n",
        "Key Reasons\n",
        "\n",
        "Native Categorical Feature Support\n",
        "CatBoost accepts categorical features in their original form. You don‚Äôt need to manually apply one-hot encoding or label encoding.\n",
        "\n",
        "Ordered Target Encoding\n",
        "It converts categories into numerical values using target statistics (like mean target), but does this in an ordered way so that future data is not used while training.\n",
        "‚ûú This prevents target leakage.\n",
        "\n",
        "Avoids One-Hot Encoding Explosion\n",
        "One-hot encoding creates many extra columns, especially for high-cardinality features.\n",
        "CatBoost avoids this, making training faster and memory-efficient.\n",
        "\n",
        "Handles High-Cardinality Categories Well\n",
        "Works efficiently with features having many unique values (e.g., user IDs, product IDs).\n",
        "\n",
        "Built-in Regularization for Categorical Features\n",
        "Reduces overfitting and improves generalization when learning from categorical data."
      ],
      "metadata": {
        "id": "PG0hJiqv0kf1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 5.What are some real-world applications where boosting techniques are\n",
        "   preferred over bagging methods?\n",
        "\n",
        "\n",
        "->Boosting techniques are preferred over bagging methods in real-world problems where high accuracy and bias reduction are more important than just variance reduction. Some common applications are:\n",
        "\n",
        "1. Fraud Detection\n",
        "\n",
        "Identifying fraudulent credit card or insurance transactions\n",
        "\n",
        "Boosting focuses on hard-to-classify cases, which are common in fraud data\n",
        "\n",
        "Algorithms used: XGBoost, CatBoost\n",
        "\n",
        "2. Customer Churn Prediction\n",
        "\n",
        "Predicting which customers are likely to leave a service\n",
        "\n",
        "Boosting captures complex patterns and interactions in customer behavior\n",
        "\n",
        "3. Search Engines & Ranking Systems\n",
        "\n",
        "Ranking web pages or products\n",
        "\n",
        "Boosting (e.g., Gradient Boosting, XGBoost) minimizes ranking loss effectively\n",
        "\n",
        "4. Recommendation Systems\n",
        "\n",
        "Product or content recommendations (e-commerce, streaming platforms)\n",
        "\n",
        "Boosting handles non-linear relationships better than bagging\n",
        "\n",
        "5. Medical Diagnosis\n",
        "\n",
        "Disease prediction from clinical data\n",
        "\n",
        "Boosting improves accuracy by focusing on misclassified patients\n",
        "\n",
        "6. Financial Risk Modeling\n",
        "\n",
        "Credit scoring and loan default prediction\n",
        "\n",
        "Boosting provides high predictive power on structured tabular data\n",
        "\n",
        "7. Click-Through Rate (CTR) Prediction\n",
        "\n",
        "Online advertising\n",
        "\n",
        "Boosting handles imbalanced and large-scale datasets well"
      ],
      "metadata": {
        "id": "FGQKDnOP1DVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to:\n",
        "#‚óè Train an AdaBoost Classifier on the Breast Cancer dataset\n",
        "#‚óè Print the model accuracy\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Create and train the AdaBoost classifier\n",
        "model = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkkWw3jS1uvX",
        "outputId": "5a313188-0b95-465e-ce3a-2212a26560ac"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.9736842105263158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to:\n",
        "#‚óè Train a Gradient Boosting Regressor on the California Housing dataset\n",
        "#‚óè Evaluate performance using R-squared score\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load the California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Create and train the Gradient Boosting Regressor\n",
        "model = GradientBoostingRegressor(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model using R-squared score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(\"R-squared Score:\", r2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SF55WUtL2Kwo",
        "outputId": "88fa1cc5-bbc8-4107-ee9e-a25ed88a8bc1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R-squared Score: 0.7756446042829697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #Write a Python program to:\n",
        "#‚óè Train an XGBoost Classifier on the Breast Cancer dataset\n",
        "#‚óè Tune the learning rate using GridSearchCV\n",
        "#‚óè Print the best parameters and accuracy\n",
        "\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Create XGBoost classifier\n",
        "xgb = XGBClassifier(\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='logloss',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Define parameter grid for learning rate\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2]\n",
        "}\n",
        "\n",
        "# Apply GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=xgb,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy'\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Model Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGSmA3UD2hdj",
        "outputId": "f9d20650-55df-484a-8c3d-28c72a192248"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [03:46:25] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [03:46:25] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [03:46:25] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [03:46:25] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [03:46:26] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [03:46:26] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [03:46:26] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [03:46:26] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [03:46:26] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [03:46:27] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [03:46:27] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [03:46:27] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [03:46:27] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [03:46:28] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [03:46:29] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [03:46:29] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [03:46:29] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [03:46:29] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [03:46:29] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [03:46:30] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [03:46:30] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'learning_rate': 0.2}\n",
            "Model Accuracy: 0.956140350877193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to:\n",
        "#‚óè Train a CatBoost Classifier\n",
        "#‚óè Plot the confusion matrix using seaborn\n",
        "# Import required libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# Load the Iris dataset (a common classification dataset)\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize and train the CatBoost Classifier\n",
        "model = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=3, verbose=False)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the confusion matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# Plot the confusion matrix using seaborn\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix for CatBoost Classifier on Iris Dataset')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "Wa9jWE1y3QQB",
        "outputId": "8c4f52ed-b2c9-417c-94fc-0ffda3c4ed04"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'catboost'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3455652253.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcatboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCatBoostClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Load the Iris dataset (a common classification dataset)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'catboost'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.You're working for a FinTech company trying to predict loan default using\n",
        "customer demographics and transaction behavior.\n",
        "The dataset is imbalanced, contains missing values, and has both numeric and\n",
        "categorical features.\n",
        "Describe your step-by-step data science pipeline using boosting techniques:\n",
        "‚óè Data preprocessing & handling missing/categorical values\n",
        "‚óè Choice between AdaBoost, XGBoost, or CatBoost\n",
        "‚óè Hyperparameter tuning strategy\n",
        "‚óè Evaluation metrics you'd choose and why\n",
        "‚óè How the business would benefit from your model\n",
        "\n",
        "\n",
        "\n",
        "->1. Data Preprocessing\n",
        "a) Handling Missing Values\n",
        "\n",
        "Numeric features\n",
        "\n",
        "Use median imputation (robust to outliers)\n",
        "\n",
        "Categorical features\n",
        "\n",
        "Replace missing values with \"Unknown\" or let the model handle them directly\n",
        "\n",
        "üí° Why boosting helps: Tree-based boosting models handle missing values better than linear models.\n",
        "\n",
        "b) Handling Categorical Features\n",
        "\n",
        "Low-cardinality features (e.g., gender, loan type):\n",
        "\n",
        "Label encoding or native model handling\n",
        "\n",
        "High-cardinality features (e.g., occupation, merchant category):\n",
        "\n",
        "Prefer CatBoost, which uses ordered target encoding safely\n",
        "\n",
        "c) Handling Class Imbalance\n",
        "\n",
        "Loan default is usually rare.\n",
        "\n",
        "Techniques:\n",
        "\n",
        "Use class weights (scale_pos_weight)\n",
        "\n",
        "Avoid random oversampling that may cause overfitting\n",
        "\n",
        "Boosting naturally focuses more on hard-to-classify minority samples\n",
        "\n",
        "2. Choice of Boosting Algorithm\n",
        "Best Choice: CatBoost ü•á\n",
        "Model\tReason\n",
        "AdaBoost\tToo sensitive to noise and outliers\n",
        "XGBoost\tPowerful but requires manual encoding\n",
        "CatBoost\tHandles categorical features & missing values natively\n",
        "\n",
        "‚úÖ CatBoost is ideal for structured financial data with mixed feature types and imbalance.\n",
        "\n",
        "3. Hyperparameter Tuning Strategy\n",
        "Step-by-step tuning\n",
        "\n",
        "Start with default parameters\n",
        "\n",
        "Tune critical parameters:\n",
        "\n",
        "learning_rate\n",
        "\n",
        "depth\n",
        "\n",
        "iterations\n",
        "\n",
        "l2_leaf_reg\n",
        "\n",
        "Use:\n",
        "\n",
        "GridSearchCV (small search space)\n",
        "\n",
        "RandomizedSearchCV (large datasets)\n",
        "\n",
        "Use cross-validation to avoid overfitting"
      ],
      "metadata": {
        "id": "7k1WdF1V5SX8"
      }
    }
  ]
}